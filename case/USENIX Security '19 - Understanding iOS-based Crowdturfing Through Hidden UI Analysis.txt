thank you for the introduction. my talk today will be about understand West Beach Carter fing through hidden UI analysis. this work was done in collaboration with Indiana University and picking University. while we use our phone or computers, we meet a lot of fake information, fake reviews, fake news, fake accounts, fake app reviews are good examples. such fake information are propagated through a model called crowd turfing. crowd turfing is basically malicious crowd sourcing. it is illicit business model in which cyber criminals recruit small-time workers to carry out malicious tasks for dishonest third parties. in other words, there they just hire a large number of users to perform malicious tests. online crowd surfing is supported by a crowd trafficking platform which consists of a server and a client, as shown in the figure. as mentioned, cyber criminals takes requests from the dishonest third parties and pushes tasks to clients, which is further executed by small-time workers. prior research have studied web-based router f---ing, which only utilizes desktop clients. however, we are the first to study mobile client based car turfing. to the best of our knowledge, mobile crowd surfing is becoming more important as mobile uses is on the rise. according to recent reports, 58% of network visits are from mobile devices in 2018. while web-based crowd surfing platforms are simply hosted on websites. mobile crowd surfing requires an app to be installed in the user&#39;s device to successfully accomplish the malicious tasks. it is very important for the cyber criminals to reach a large number of underground workers. however, such crowd surfing apps are banned from popular app stores such as Google Play or Apple App Store. when ster reported, they&#39;re immediately removed from the App Store. in this research, we found crowd surfing apps that still infiltrate the app store. they basically hide the cryo turfing UI behind a benign looking light app during the app review process. the screenshot and the slide is a good example. it is a typical cloud turfing app. left side is a word game UI, the first is the game list and the second is the game- the actual game UI. the app description only mentions the word game. the right side is a car turfing UI. it is a list of tasks that can be performed by the user. it is only shown under when specific conditions are met. the goal of this project is to first develop a triage methodology to identify crowd-surfing apps. secondly, we also aim to provide new understanding on mobile crowd turfing ecosystem. as we just saw, crowd turfing UI are hidden inside benign apps and only get triggered when specific conditions are met. even when they show up, their functionality are very similar to benign. like benign apps, they don&#39;t download an email moer, they don&#39;t download any malicious payload, they don&#39;t use any sensitive api or a private API. this makes it very difficult to use traditional signature-based methodology to detect them. a simple approach is to let human actually look at the content and decide whether if it&#39;s related to crowd surfing. however, that is not feasible during the app vetting process we built- or because of that we built- our detection approach based on the two key ideas we make you the fact that crowd Rafi, new I are conditionally triggered and are semantically related to crowd turfing. let&#39;s take a closer look with an example. the left side is a code that we got from a real crowd turing app and the right side is a labeled view control graph that we used to analyze the UI transitioning of the app. as you can see in the first green box, the app, when it received a scheme, after parsing it, it&#39;ll decide what value is just set to us or in variable. and when the app is opened, depending on what type of error value is set to the variable, it&#39;ll trigger either the crowd-surfing UI or the benign UI and, as you can see. the first set of keywords extracted from the crowd trapping UI is obviously related to crowd turfing: task cache, earn, withdrawal, join, paper work. the second set of keywords- album, singer, shuffle, song, music, radio- is obviously related to music player. more details about the labeled view control graph can be found in our paper. using such observation, we came up with our detection tool, cruiser, which consists of a structure, a minor and a semantic analyzer. the structure minor takes an app downloaded from the App Store, either input, disassembles it and analyzes its UI transitioning using the label view controller graphic that I just shown. after analyzing the label view control graph, it&#39;ll come up with a set of suspicious UI. when we see suspicious UI, we mean potential crowd turfing UI. once the suspicious UI are given, the semantic analyzer will analyze the semantics of the suspicious UI to confirm whether if it&#39;s a crowd riffing UI or not. the structure minor is built based on the fact that the set of crowd riffing you- I are totally different from the set of benign UI. we search for apps that have more than one entry you ice. if there are more than one entry UI, the main UI can be changed based on certain conditions. we also look for uisce that are not triggered by users intermediate you ice- that are triggered by certain conditions, for example a URL scheme, are considered as suspicious UI. let&#39;s take a look at the semantic analyzer. given the suspicious UI as an input, the semantic analyzer focuses on deciding whether the suspicious UI are indeed crowd-surfing UI by doing a simple similarity check. for that purpose, we extract the keywords from the suspicious who I said that we received from the structure minor. we apply clustering algorithms, purifying techniques and finally apply work to vector changes, to vector space. for similarity check on the other side, to get the keyword set of crowd turfing, we curled the keywords from popular crowd surfing websites, also applied topic modeling algorithms and did purification and then applied wort effect. then, after having the vectors of both sides, we did a simple similar similarity check using cosine distance. you think those approach. we confirm whether if it&#39;s proud, proud turf in you or not. to evaluate the cruiser, we use the good set and the Bassett for deciding the similarity threshold and then applied cruiser on the unknown set, which is 28 k outs. from the 28k apps, 102 apps are ported and 93 apps were actual crowd surfing apps. all 93 apps were removed from the app store immediately after reported them so far. I explained what crowd surfing is and how he detected them. now we&#39;ll be looking at the measurement state. we first will look at the crowd surfing value-chain discovered in our research. as mentioned before, there are three parties: the dishonest third parties, which requests the malicious tasks, such as app ranking manipulation, and there&#39;s the cyber criminal, the crowd surfing platform owner, and the workers who actually execute the tasks. before going to details, there are the red lines. it&#39;s gonna be meaning the money flow. the blue lines are going to be meaning their mobile crowd turfing flow and the third black line is going to be explaining the hit you at hidden UI development. besides the three major components, there are more components, such as the mobile device used by the workers, the triggering website, the promotion channel, the crowd turfing app developer and the app store to establish the crowd referring platform. the cyber criminal will first request the app to a kart riffing app developer. once the crowd surfing app is developed, they&#39;ll return the app to the cyber criminal and upload the app to the App Store, and once the app gets published, the cyber criminal will be paid. once the app is published in the app store, the cyber criminal will start to promote the apps. there are multiple promotion channels, such as online communities, social networks or app gateway sites, which I will explain later. the workers, the users who are interested in earning money through crowding, will then visit the promotion channels and install the app from the App Store to their device and visit the triggering website and turned their crowd surfing app and app into the actual crowd trip in UI, because initially it&#39;s gonna be showing only the benign UI. once the crowd turfing platform is there, it&#39;ll start. the cyber criminal will start to take requests from this dishonest third party. in this example, he&#39;s asking the cyber criminal to manipulate the app ranking and the cyber criminal will push the test. the mobile client, the workers, will execute the task, basically install the app and leave review. once the tests are verified, the cybercriminal will pay the workers. this is a summary, a quick summary of our findings. the newly discovered 93 apps were related to 67 crowd-surfing platforms. among the 93 apps, 6 apps were in the top 20 of the Apple App Store in across different countries. 14 apps were once ranked in the top 50 and 25 apps were in the top 100 of their corresponding categories. the crowd turfing apps we discovered were found from 15 categories. the FOB top 5 categories are music, utilities, lifestyle entertainment and games. while it was found across 15 app categories, most of them come from the top 5. we think it&#39;s because the crowd turfing app developers are. they do not want to spend much time on developing that- the benign part of the app, so they basically make use of open source projects or template apps found from github or blogs, I guess, to understand how car turfing platform owners spread such apps and recruit workers, we searched the app names that we discovered in our research on Google search engine. we just threw that approach. we discovered 50 channels that advertised crowd-surfing apps. most of them are, as I mentioned before, online communities, social networks and gateway sites. gateway sites are basically crowd turfing app aggregation website, but it&#39;s not a complete list and if you&#39;re interested, please don&#39;t lean. you know I can talk about it at breaktime. also, based on the download number of downloads recorded in the crowd surfing gateway websites, we found that the crowd surfing apps were downloaded more than 30- 2.4 million times. also, 23% of the apps discovered include an in-app referral feature. the user is paid for referring the cart reffing app to other users. the following are the top six crowd-surfing tasks found in a research, as we can see here, while most of them are similar to web-based crowd surfing. the first one, ranking manipulation, is included in most mobile client apps. this makes sense because app ranking manipulation can only be done through a mobile client because the app actually needs to be installed in the phone and sometimes they request them to even leave a review. also, while most have few updates, still a large portion of them were reaching a high version number. this is interesting because every time they submit a new version, they need to go through the review process again. this means that Apple does not have a systematic way to detect such apps yet. to measure the task volume of the app, we crawled five apps, task information manually and the number of required workers through their car tracking you ice. by multiplying the tasks- daily task volume to the per task price, we can get a number of 5000- $889. so the lower bound of the crowd turfing platform owner, the cybercriminal, will be five thousand eight hundred eighty nine dollars. we also studied how such apps were hiding their UI. let&#39;s begin with how they are developed: the. by searching google with the app names, we found that the crowd surfing platform owner is the cybercriminals were willing to pay up to four hundred fifty dollars for getting an illicit app that is capable of hiding the crowd surfing UI to pass the app review process. crowd surfing apps were often built based on an open source project, as mentioned before. such templates can be found in blogs or github. interestingly, six apps were based on the same music player template and there are some other findings that I will just shortly due to time limit. we also study the triggers. the UI was triggered based on logic bomb CG server schemes. many apps were resubmitted only after changing their bundle ID under it, under a different developer ID. also crowd-surfing platforms on multiple crowd riffing apps. for more details, please read the paper takeaways. we tried to provide new understanding on mobile crowd riffing ecosystem. we also provide new techniques based on hidden UI for evading app without betting, thank you. thank you, eunjin, for fascinating work. please step up, ask questions, state your name and affiliation. Anatoly, I a part of me. can you speak louder please? oh, I&#39;m Anatoly ben-gurion University, Israel. thank you for the talk. I was wondering you had mentioned in 9 apps that you were figured out that false, positive out a hundred of - this is a huge number for malicious, for wrong identification. have you tried to analyze, while that considered such a crowd laughing applications. so one reason why we call they told a triage methodologies because of the false detection rate instead of a detecting tool. and the false positives were also a lot of. a lot of them were related to malicious activities. they were not hiding crowd turfing functions, but they were hiding phishing UI or they were hiding some other malicious activities that are currently under work in our follow-up work, actually. so so those false positives are not related to power tripping, but it&#39;s still related to some malicious. so you actually don&#39;t search for the. it&#39;s a undergoing work and okay, thank you them and we&#39;ll probably some hit it soon. yeah, hello, I&#39;m Steven Chevy from Middle Tennessee State University. you mentioned that the apps don&#39;t use any sort of private API. is that indicating they&#39;re not communicating with some sort of central control server from the code turfing people? or is it that, like these texts are embedded in app updates, which may explain the high update frequency? so I think I didn&#39;t understand your question clearly because it was a little bit not loud enough. oh, sorry, the. you mentioned that they do not use private API, right? private does that mean that they&#39;re not submitting tasks to a private API server that they own. oh no, no, I&#39;m talking about the private API and the sensitive API so that are used by malicious apps in Android or iOS, because in malware research, when we talk about private APN, sensitive API, this usually API is that access sensitive data and the in the phone, like, for example, like the contacts or the location or etc. okay, thank you. okay, so let&#39;s thank all our speakers on Fabio Yi and Yong Jun. thank again.